{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Classifying Diabetic Retinopathy using Decision Trees\n",
    "---\n",
    "\n",
    "#### Attribute Information:\n",
    "- The binary result of quality assessment:\n",
    "    0 = bad quality 1 = sufficient quality.\n",
    "- You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log2\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "CLASS_LABEL = 19\n",
    "ONE_HUNDRED = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __str__(self):\n",
    "        return \"< \" + str(self.label) + \": \" + str(self.features) + \" >\"\n",
    "    \n",
    "    def __init__(self, label, features):\n",
    "        self.label = label                  # the classification label of this data point\n",
    "        self.features = features            # a list of feature values for this data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Read data from a CSV file. Put it into a list of `DataPoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "\n",
    "    messidor_df = pd.read_csv(filename, delimiter=',', header=None)\n",
    "\n",
    "    # iterate over rows using itterrows() and create DataPoint objects\n",
    "    for index, row in messidor_df.iterrows():\n",
    "        label = row.iloc[CLASS_LABEL]\n",
    "        features = row.iloc[:-1].tolist() # exclude last column (the class label)\n",
    "        data_point = DataPoint(label, features)\n",
    "        data.append(data_point)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    is_leaf = True          # boolean variable to check if the node is a leaf\n",
    "    feature_idx = None      # index that identifies the feature\n",
    "    thresh_val = None       # threshold value that splits the node\n",
    "    prediction = None       # prediction class (only valid for leaf nodes)\n",
    "    left_child = None       # left TreeNode (all values < thresh_val)\n",
    "    right_child = None      # right TreeNode (all values >= thresh_val)\n",
    "    \n",
    "    def printTree(self, level=0):    # for debugging purposes\n",
    "        if self.is_leaf:\n",
    "            print ('-'*level + 'Leaf Node:      predicts ' + str(self.prediction))\n",
    "        else:\n",
    "            print ('-'*level + 'Internal Node:  splits on feature ' \n",
    "                   + str(self.feature_idx) + ' with threshold ' + str(self.thresh_val))\n",
    "            self.left_child.printTree(level+1)\n",
    "            self.right_child.printTree(level+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recursive implementation\n",
    "def make_prediction(tree_root, data_point):\n",
    "    # base case\n",
    "    if tree_root.is_leaf:\n",
    "        return tree_root.prediction\n",
    "    \n",
    "    # otherwise, lets check the feature value\n",
    "    feature_value = data_point.features[tree_root.feature_idx]\n",
    "\n",
    "    # now we split depending on the tree's threshold value\n",
    "    if feature_value < tree_root.thresh_val:\n",
    "        return make_prediction(tree_root.left_child, data_point)\n",
    "    else:\n",
    "        return make_prediction(tree_root.right_child, data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(data, feature_idx, threshold):\n",
    "    left_split = []\n",
    "    right_split = []\n",
    "\n",
    "    for data_point in data:\n",
    "        feature_value = data_point.features[feature_idx]\n",
    "        if feature_value < threshold:\n",
    "            left_split.append(data_point)\n",
    "        else:\n",
    "            right_split.append(data_point)\n",
    "        \n",
    "    return (left_split, right_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Entropy(data) = \\sum_{n=1} ^{c} -p_n \\log_2 p_n\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    positive_cases = 0\n",
    "    negative_cases = 0\n",
    "    total_cases = 0\n",
    "\n",
    "    # first calculate the frequency for each class label in the data\n",
    "    for data_point in data:\n",
    "        if data_point.label == 1:\n",
    "            positive_cases += 1\n",
    "        else:\n",
    "            negative_cases += 1\n",
    "        total_cases += 1\n",
    "\n",
    "    # now we can calculate entropy for each label\n",
    "    entropy = 0.0\n",
    "    pos_calc = 0\n",
    "    neg_calc = 0\n",
    "    if positive_cases == 0:\n",
    "        pos_calc = 0\n",
    "    elif negative_cases == 0:\n",
    "        neg_calc = 0\n",
    "    else:\n",
    "        pos_probability = positive_cases / total_cases\n",
    "        neg_probability = negative_cases / total_cases\n",
    "        pos_calc = pos_probability * log2(pos_probability)\n",
    "        neg_calc = neg_probability * log2(neg_probability)\n",
    "\n",
    "    entropy = -(pos_calc) - (neg_calc)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Gain_{split} = Impurity(parent) - (\\sum_{n=1} ^{k} w_n Impurity(n))\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_best_threshold(data, feature_idx):\n",
    " \n",
    "    # custom function to calc gain split using entropy\n",
    "    def calc_gain_split(data, feature_idx, threshold):\n",
    "        parent_entropy = calc_entropy(data)\n",
    "        left_split, right_split = split_dataset(data, feature_idx, threshold)\n",
    "        left_entropy = calc_entropy(left_split)\n",
    "        right_entropy = calc_entropy(right_split)\n",
    "        left_weight = len(left_split) / len(data)\n",
    "        right_weight = len(right_split) / len(data)\n",
    "        return parent_entropy - ((left_weight * left_entropy) + (right_weight * right_entropy))\n",
    "\n",
    "    best_gain_split = -1\n",
    "    best_thresh = -1\n",
    "    feature_set = set()\n",
    "    \n",
    "    # obtain set of unique features from given DataPoint\n",
    "    for data_point in data:\n",
    "        feature_set.add(data_point.features[feature_idx])\n",
    "    ordered_features = sorted(feature_set)\n",
    "    \n",
    "    # __GAIN SPLIT AND THRESHOLD CALCULATIONS__\n",
    "    for i in range(len(ordered_features) - 1):\n",
    "        cur_val = ordered_features[i]\n",
    "        next_val = ordered_features[i + 1]\n",
    "        threshold = (cur_val + next_val) / 2.0\n",
    "        gain_split = calc_gain_split(data, feature_idx, threshold)\n",
    "        \n",
    "        # update best_gain_split and best_threshold if needed\n",
    "        if gain_split > best_gain_split:\n",
    "            best_gain_split = gain_split\n",
    "            best_thresh = threshold\n",
    "\n",
    "    return (best_gain_split, best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_best_split(data):\n",
    "    if len(data) < 2:\n",
    "        return (None, None)\n",
    "    best_feature = None\n",
    "    best_thresh = None\n",
    "    best_gain = 0\n",
    "    num_features = len(data[0].features)\n",
    "    \n",
    "    # now lets iterate through each feature from our data and see which feature give us the best total gain\n",
    "    for feature_idx in range(num_features):\n",
    "        gain, threshold = calc_best_threshold(data, feature_idx)\n",
    "        # update best_gain if needed \n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature_idx\n",
    "            best_thresh = threshold\n",
    "            \n",
    "    return (best_feature, best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf_node(data):\n",
    "    leaf_node = TreeNode()\n",
    "    leaf_node.is_leaf = True\n",
    "\n",
    "    positive_cases = 0\n",
    "    negative_cases = 0\n",
    "\n",
    "    # first calculate the frequency for each class label in the data\n",
    "    for data_point in data:\n",
    "        if data_point.label == 1:\n",
    "            positive_cases += 1\n",
    "        else:\n",
    "            negative_cases += 1\n",
    "\n",
    "    # Special case\n",
    "    if positive_cases == negative_cases:\n",
    "        leaf_node.prediction = 1\n",
    "    elif positive_cases > negative_cases:\n",
    "        leaf_node.prediction = 1\n",
    "    else:\n",
    "        leaf_node.prediction = 0\n",
    "        \n",
    "    return leaf_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_decision_tree(data, max_levels):\n",
    "    # base case\n",
    "    if max_levels <= 1:\n",
    "        return create_leaf_node(data)\n",
    "\n",
    "    feature_idx, threshold = identify_best_split(data)\n",
    "    \n",
    "    # special case: no impurity (entropy = 0)\n",
    "    if feature_idx == None:\n",
    "        return create_leaf_node(data)\n",
    "\n",
    "    # create an internal node at this point in the recursion\n",
    "    internal_node = TreeNode()\n",
    "    internal_node.feature_idx = feature_idx\n",
    "    internal_node.thresh_val = threshold\n",
    "    internal_node.is_leaf = False\n",
    "\n",
    "    # create internal node's children and then recurse\n",
    "    left_data, right_data = split_dataset(data, feature_idx, threshold)\n",
    "    internal_node.left_child = create_decision_tree(left_data, max_levels - 1)\n",
    "    internal_node.right_child = create_decision_tree(right_data, max_levels - 1)\n",
    "    \n",
    "    return internal_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(tree_root, test_data):\n",
    "    correct_predictions = 0\n",
    "    for data_point in test_data:\n",
    "        actual_label = data_point.label\n",
    "        predicted_label = make_prediction(tree_root, data_point)\n",
    "        if actual_label == predicted_label:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    return correct_predictions / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "Time taken: 3.0063350200653076\n",
      "The accuracy on the test set is  63.91304347826087\n",
      "\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "Time taken: 2.7375922203063965\n",
      "The accuracy on the test set is  63.04347826086957\n",
      "\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "Time taken: 2.7224597930908203\n",
      "The accuracy on the test set is  66.95652173913044\n",
      "\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "Time taken: 2.839056968688965\n",
      "The accuracy on the test set is  63.91304347826087\n",
      "\n",
      "Training set size: 920\n",
      "Test set size    : 230\n",
      "Time taken: 2.7841410636901855\n",
      "The accuracy on the test set is  64.34782608695652\n",
      "\n",
      "The Average Accuracy on a 5-fold Cross Validation: 64.43478260869566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "filename = 'messidor_data.txt'\n",
    "data = get_data(filename)\n",
    "\n",
    "# partition data into train_set and test_set\n",
    "k_folds = 5\n",
    "fold_size = len(data) // k_folds\n",
    "total_fold_accuracies = 0.0\n",
    "\n",
    "# test model \n",
    "for i in range(k_folds):\n",
    "    start_idx = i * fold_size\n",
    "    end_idx = (i + 1) * fold_size \n",
    "    train_set = data[:start_idx] + data[end_idx:]\n",
    "    test_set = data[start_idx:end_idx]\n",
    "\n",
    "    print('Training set size:', len(train_set))\n",
    "    print('Test set size    :', len(test_set))\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # create the decision tree\n",
    "    tree = create_decision_tree(train_set, 10)\n",
    "\n",
    "    end = time.time()\n",
    "    print ('Time taken:', end - start)\n",
    "\n",
    "    # calculate the accuracy of the tree\n",
    "    accuracy = calc_accuracy(tree, test_set)\n",
    "    total_fold_accuracies += accuracy\n",
    "    print('The accuracy on the test set is ', str(accuracy * ONE_HUNDRED))\n",
    "    print()\n",
    "\n",
    "avg_accuracy = (total_fold_accuracies / k_folds) * ONE_HUNDRED\n",
    "print(f'The Average Accuracy on a 5-fold Cross Validation: {avg_accuracy}\\n',)\n",
    "#tree.printTree() # for debugging purposes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
